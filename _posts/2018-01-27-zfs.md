---
title: FreeBSD - zfs
tags: [zfs, freebsd]
reference: 
  - 
    link: "https://docs.oracle.com/cd/E19253-01/820-0836/zfsover-2/index.html"
    title: "Что такое ZFS"
  -
    link: "https://docs.oracle.com/cd/E19253-01/820-0836/gaztn/index.html"
    title: "Точки монторивания"
  -
    link: "https://youtu.be/ZhlkwTNkh58"
    title: "ZFS на Linux Debian 8. Часть 1"
  -
    link: "https://youtu.be/IJ1VNmjOZhE"
    title: "ZFS на Linux Debian 8. Часть 2"
  -
    link: "https://www.youtube.com/watch?v=j06efTacH3E&t=36s"
    title: "Установка FreeBSD+ZFS"
  -
    link: "https://confluence.prototypes.ru/x/C4-/"
    title: "confluence"
  -
    link: "https://deathstar.name/ustanovka-freebsd-na-zfs-vklyuchaya-kornevoj-razdel/"
    title: "Установка FreeBSD на ZFS включая корневой раздел"
  -
    link: "1%20http://www.lissyara.su/articles/freebsd/file_system/root_zfs_gpt/"
    title: "Установка FreeBSD на ZFS/root или Пособие себе на память"
  -
    link: "http://dreamcatcher.ru/2010/01/10/%D0%A8%D0%BF%D0%B0%D1%80%D0%B3%D0%B0%D0%BB%D0%BA%D0%B0-%D0%BF%D0%BE-zfs/"
    title: "Шпаргалка-по-zfs"
  -
    link: "http://xgu.ru/wiki/ZFS"
    title: "Пример использования ZFS"
---

* TOC 
{:toc}

### zpool
zpool [действие] [ключи]

<table>
    <tr>
        <th>Ключ</th>
        <th>Описание</th>
    </tr>
    <tr>
        <td>-f, --force</td>
        <td></td>
    </tr>
    <tr>
        <td>-d</td>
        <td>Отключить все фичи по умолчанию</td>
    </tr>
    <tr>
        <td>-o property=value</td>
        <td>
            включить <a href="#features">фичу</a>
        </td>
    </tr>
    <tr>
        <td>-O file-system-property=value</td>
        <td>
            <a href="#settings_fs">Настройки файловой системы</a>
        </td>
    </tr>
    <tr>
        <td>-m</td>
        <td>
            <p>точка монторования</p>
            <p>Если ее нет, то нужно выбрать legacy или none</p>
            <ol>
                <li>none - без монтирования</li>
                <li>legacy - как я понимаю это наследование уровневых путей. например /export/stuff, то pool/home/user путем наследования получает значение /export/stuff/user
                    <p>Старые точки монтирования. Управление файловыми системами ZFS можно осуществлять при помощи старых средств. Для этого свойство mountpoint должно иметь значение legacy. Управление старыми файловыми системами должно осуществляться с помощью команд mount и umount и файла /etc/vfstab. ZFS не выполняет автоматическое монтирование старых файловых систем при начальной загрузке, и для наборов данных этого типа команды ZFS mount и umount не используются. В следующих примерах демонстрируется настройка и администрирование набора данных ZFS в режиме "legacy":</p>
                </li>
            </ol>
        </td>
    </tr>
</table>

### Шпаргалка по командам

<table>

<!-- ------------------------------------------------------------- -->

<tr><td>
    
    <p>Справка по командам</p>

</td><td>

<pre><code class="shell">man zpool
man zfs
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>
    
    <h5 id="zpool create">zpool create</h5>
    <p>Создать пул и посмотреть информацию о нём.</p>
    <p>Интересная особенность в *nix системах - <a href="http://linux.yaroslavl.ru/docs/setup/mandrake/cl/ch09s02.html">Все является файлом</a>. В том числе и диски. Поэтому, пул дисков можно эмулировать с помощью создания файлов</p>
    <p>Единственное ограничение, диск (или файл) должен быть не менее 64m для самого zfs</p>
    <p>При создании пула, создается фс от корня по названию пула</p>

</td><td>

<pre><code class="shell">
# cd /
# mkfile 100m disk1 disk2 disk3
# ls -l disk*
-rw-------  1 root  wheel  104857600 Jan 27 14:42 disk1
-rw-------  1 root  wheel  104857600 Jan 27 14:42 disk2
-rw-------  1 root  wheel  104857600 Jan 27 14:42 disk3
-rw-------  1 root  wheel   73400320 Jan 20 19:58 disk4
# zpool create test01 /disk1 /disk2
# zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test01   160M    62K   160M         -     1%     0%  1.00x  ONLINE  -
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zpool status">zpool status</h5>
    <p>Подробная информация о пуле</p>
    <p>Пока не увидел разницу от ключа -v</p>

</td><td>

<pre><code class="shell">
# zpool status -v
  pool: test01
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    test01      ONLINE       0     0     0
      /disk1    ONLINE       0     0     0
      /disk2    ONLINE       0     0     0

errors: No known data errors</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zpool destroy">zpool destroy</h5>
    <p>Уничтожить пул</p>
    <p>При уничтожении пула, уничтожается и фс!</p>

</td><td>

<pre><code class="shell">
# zpool destroy test01
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>
        <h5 id="zpool create mirror">zpool create mirror</h5>
        <p>Создать зеркалированный пул</p>

</td><td>

<pre><code class="shell">
# zpool create test02 mirror /disk1 /disk2 
# zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test02    80M   242K  79.8M         -     1%     0%  1.00x  ONLINE  -
</code></pre>

<pre><code class="shell">
# mkfile 70m disk4
# zpool create test03 mirror /disk3 /disk4 
invalid vdev specification
use '-f' to override the following errors:
mirror contains devices of different sizes
</code></pre>

<p>При создании зеркала разного размера, выдется ошибка</p>
<p>В этом случае можно указать ключ -f для принудительного создания. Тогда размер пула будет наименьшим из дисков</p>

<pre><code class="shell">
# zpool create -f test03 mirror /disk3 /disk4 
# zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test03    64M    62K  63.9M         -     1%     0%  1.00x  ONLINE  -
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zpool detach">zpool detach</h5>
    <p>Отключить диск от зеркала</p>

</td><td>

<pre><code class="shell">
# zpool status
  pool: test02
 state: ONLINE
  scan: resilvered 70K in 0h0m with 0 errors on Sat Jan 20 18:18:35 2018
config:

    NAME        STATE     READ WRITE CKSUM
    test02      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        /disk2  ONLINE       0     0     0
        /disk1  ONLINE       0     0     0

errors: No known data errors
# zpool detach test02 /disk1
# zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test02    80M   124K  79.9M         -     1%     0%  1.00x  ONLINE  -
# zpool status
  pool: test02
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    test02      ONLINE       0     0     0
      /disk2    ONLINE       0     0     0

errors: No known data errors
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zpool attach">zpool attach</h5>
    <p>Добавить диск к зеркалу</p>
    <p>Нужно передовать 2 диска, с какого на какой скопируются данные для дальнейшего зеркалирования</p>

</td><td>

<pre><code class="shell">
# zpool attach test02 /disk2 /disk1
# zpool status
  pool: test02
 state: ONLINE
  scan: resilvered 70K in 0h0m with 0 errors on Sat Jan 20 18:18:35 2018
config:

    NAME        STATE     READ WRITE CKSUM
    test02      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        /disk2  ONLINE       0     0     0
        /disk1  ONLINE       0     0     0

errors: No known data errors
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr>
    <td>
        
        <h5 id="zpool replace">zpool replace</h5>
        <p>Заменить диск</p>
</td><td>
        <pre><code class="shell">
        # zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test01   160M   256K   160M         -     1%     0%  1.00x  ONLINE  -
# zpool status
  pool: test01
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    test01      ONLINE       0     0     0
      /disk1    ONLINE       0     0     0
      /disk2    ONLINE       0     0     0

errors: No known data errors
# zpool replace test01 /disk1 /disk3 
# zpool status
  pool: test01
 state: ONLINE
  scan: resilvered 27K in 0h0m with 0 errors on Sat Jan 27 16:15:06 2018
config:

    NAME        STATE     READ WRITE CKSUM
    test01      ONLINE       0     0     0
      /disk3    ONLINE       0     0     0
      /disk2    ONLINE       0     0     0

errors: No known data errors
        </code></pre>

    </td>
</tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zpool add spare">zpool add spare</h5>
    <p>Добавить запасное устройство</p>
    <p>Автоматическая замена вышедшего из строя устройства. TODO: проверить</p>

</td><td>

<pre><code class="shell">zpool add test01 spare /disk3 
# zpool status
  pool: test01
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    test01      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        /disk1  ONLINE       0     0     0
        /disk2  ONLINE       0     0     0
    spares
      /disk3    AVAIL   

errors: No known data errors
</code></pre>
</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <p>Заменить диск на запасной</p>

</td><td>

<pre><code class="shell">zpool replace test01 /disk1 /disk3 
# zpool status
  pool: test01
 state: ONLINE
  scan: resilvered 71.5K in 0h0m with 0 errors on Sat Jan 27 18:03:57 2018
config:

    NAME                   STATE     READ WRITE CKSUM
    test01                 ONLINE       0     0     0
      mirror-0             ONLINE       0     0     0
        spare-0            ONLINE       0     0     0
          /disk1           ONLINE       0     0     0
          /disk3           ONLINE       0     0     0
        /disk2             ONLINE       0     0     0
    spares
      9988242779740387816  INUSE     was /disk3

errors: No known data errors
# zpool detach test01 /disk1
# zpool status
  pool: test01
 state: ONLINE
  scan: resilvered 74.5K in 0h0m with 0 errors on Sat Jan 27 18:07:32 2018
config:

    NAME        STATE     READ WRITE CKSUM
    test01      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        /disk3  ONLINE       0     0     0
        /disk2  ONLINE       0     0     0

errors: No known data errors
</code></pre>
</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zpool remove">zpool remove</h5>
    <p>Удалить запасное устройство</p>

</td><td>

<pre><code class="shell">
# zpool remove test01 /disk4
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

        <h5 id="zpool offline">zpool offline</h5>
        <p>Вывести указанное устройство из эксплуатации</p>
        <p>После этого попыток писать и читать это устройство не будет до тех пор, пока оно не будет переведено в online. Если использовать ключ -t, устройство будет переведено в offline временно. После перезагрузки устройство опять будет в работе (online).</p>
</td><td>

        <pre><code class="shell">
      # zpool status
pool: test01
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    test01      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        /disk1  ONLINE       0     0     0
        /disk2  ONLINE       0     0     0

errors: No known data errors
# zpool offline test01 /disk2
# zpool status
  pool: test01
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
    Sufficient replicas exist for the pool to continue functioning in a
    degraded state.
action: Online the device using 'zpool online' or replace the device with
    'zpool replace'.
  scan: none requested
config:

    NAME                      STATE     READ WRITE CKSUM
    test01                    DEGRADED     0     0     0
      mirror-0                DEGRADED     0     0     0
        /disk1                ONLINE       0     0     0
        14576361860791797662  OFFLINE      0     0     0  was /disk2

errors: No known data errors
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

        <h5 id="zpool online">zpool online</h5>
        <p>Включить диск</p>

</td><td>

<pre><code class="shell">
# zpool online test01 /disk1
# zpool status
  pool: test01
 state: ONLINE
  scan: resilvered 4.50K in 0h0m with 0 errors on Sat Jan 20 18:49:21 2018
config:

    NAME        STATE     READ WRITE CKSUM
    test01      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        /disk1  ONLINE       0     0     0
        /disk2  ONLINE       0     0     0

errors: No known data errors
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zpool scrub">zpool scrub</h5>
    <p>Выполнить скраббинг «чистку» хранилища.</p>
    <p>Для того чтобы знать, что там все контрольные суммы верны. Если используется зеркало или RAIDZ автоматически восстанавливаются сбойные блоки.</p>

</td><td>

<pre><code class="shell">/test01 # zpool scrub test01
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zpool export">zpool export</h5>
    <p>Экспортировать пул для импорта в другую систему</p>

</td><td>

<pre><code class="shell">
# zpool export test01
# zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test02   160M   128M  31.9M         -    60%    80%  1.00x  ONLINE  -
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zpool import">zpool import</h5>
    <p>Импортировать пул</p>
    <p>Если ключ -d не указан, команда ищет /dev/dsk.</p>
    <p>Поскольку в примере используются файлы нужно указать каталог где они лежат</p>

</td><td>

<pre><code class="shell">
# zpool import -d / test01
# zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test01    80M  48.1M  31.9M         -    45%    60%  1.00x  ONLINE  -
test02   160M   128M  31.9M         -    60%    80%  1.00x  ONLINE  -
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

        <h5 id="zpool upgrade">zpool upgrade</h5>
        <p>Показать версию формата пулов</p>
        <p>У меня не работает :(</p>

</td><td>

<pre><code class="shell">
# zpool upgrade
This system supports ZFS pool feature flags.

All pools are formatted using feature flags.

Every feature flags pool has all supported features enabled.
</code></pre>

        <p>Ключ -v говорит, что нужно показать какие функции поддерживаются текущей версией.</p>
<pre><code class="shell">
# zpool upgrade -v
This system supports ZFS pool feature flags.

The following features are supported:

FEAT DESCRIPTION
-------------------------------------------------------------
async_destroy                         (read-only compatible)
...
The following legacy versions are also supported:

VER  DESCRIPTION
---  --------------------------------------------------------
 1   Initial ZFS version
 2   Ditto blocks (replicated metadata)
 3   Hot spares and double parity RAID-Z
 4   zpool history
 ...

</code></pre>

<p>Ключ -a говорит, что нужно обновить до последней версии.</p>

<pre><code class="shell">
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zpool iostat">zpool iostat</h5>
    <p>Посмотреть статистику ввода/вывода</p>

</td><td>

<pre><code class="shell">
# zpool iostat 1
               capacity     operations    bandwidth
pool        alloc   free   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
test01       234K  79.8M      0      0     85    336
test01       234K  79.8M      0      0      0      0
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

        <h5 id="zfs create">zfs create</h5>
        <p>Создать файловую систему</p>
        <p>Проверить её с помощью df -h. Файловая система автоматически монтируется в /test01.</p>

</td><td>

<pre><code class="shell">
# zfs list
NAME     USED  AVAIL  REFER  MOUNTPOINT
test01  68.5K   128M    19K  /test01
# zfs create test01/test02
# zfs list
NAME            USED  AVAIL  REFER  MOUNTPOINT
test01           92K   128M    19K  /test01
test01/test02    19K   128M    19K  /test01/test02
/ # df -h
Filesystem       Size    Used   Avail Capacity  Mounted on
/dev/ada0p2      9.0G    4.5G    3.8G    55%    /
devfs            1.0K    1.0K      0B   100%    /dev
test01           128M     19K    128M     0%    /test01
test01/test02    128M     19K    128M     0%    /test01/test02
</code></pre>


</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <p>Создание ещё одной файловой системы. Обратите внимание, что обе файловые системы как будто бы имеют 128M свободных, ибо квоты не установлены. Каждая может расти до тех пор, пока не заполнит пул:</p>

</td><td>

<pre><code class="shell">
# zfs list
NAME            USED  AVAIL  REFER  MOUNTPOINT
test01          112K   128M    19K  /test01
test01/test02    19K   128M    19K  /test01/test02
test01/test03    19K   128M    19K  /test01/test03
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zfs list">zfs list</h5>
    <p>Список zfs файловых систем</p>

</td><td>

<pre><code class="shell">
# zfs list
NAME            USED  AVAIL  REFER  MOUNTPOINT
test01         48.1M      0    19K  /test01
test01/test02  47.9M      0  47.9M  /test01/test02
test02          128M      0   128M  /test02
</code></pre>

<p>Также можно посмотреть командой df</p>
<pre><code class="shell">
# df -h
Filesystem       Size    Used   Avail Capacity  Mounted on
/dev/ada0p2      9.0G    4.5G    3.7G    55%    /
devfs            1.0K    1.0K      0B   100%    /dev
test02           128M    128M      0B   100%    /test02
test01            19K     19K      0B   100%    /test01
test01/test02     48M     48M      0B   100%    /test01/test02
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zfs set reservation">zfs set reservation</h5>
    <p>Установка резервирования фс</p>

    <a href="#3">Проверка</a>

</td><td>

<pre><code class="shell">
# zfs set reservation=20m test01/test02
# zfs list -o reservation
RESERV
  none
   20M
  none
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zfs set quota">zfs set quota</h5>
    <p>Установить и показать квоты.</p>
    <p>Максимальное занимаемое место</p>

</td><td>

<pre><code class="shell">
# zfs set quota=20m test02/test03
root@freebsd2:~ # zfs list -o quota
QUOTA
 none
  20M
# cat /dev/urandom > /test02/test03/file2
cat: stdout: Disc quota exceeded
root@freebsd2:/test02/test03 # ls -lh
total 20499
-rw-r--r--  1 root  wheel    20M Jan 27 20:09 file2
# touch /test02/test03/file3
touch: /test02/test03/file3: Disc quota exceeded
</code></pre>

<p>Но ее можно увеличить</p>

<pre><code class="shell">
# zfs set quota=30m test02/test03
# cat /dev/urandom > /test02/test03/file3
cat: stdout: Disc quota exceeded
# ls -lh /test02/test03/
total 30747
-rw-r--r--  1 root  wheel    20M Jan 27 20:09 file2
-rw-r--r--  1 root  wheel    10M Jan 27 20:13 file3
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zfs set compression">zfs set compression</h5>
    <p>Включить сжатие</p>

    <a href="#4">Проверка</a>

</td><td>

<pre><code class="shell">
# zfs set compression=on test02/test03
root@freebsd2:~ # cat /dev/urandom > /test02/test03/file1
cat: stdout: No space left on device
root@freebsd2:~ # ls -lh /test02/test03/
total 130914
-rw-r--r--  1 root  wheel   128M Jan 27 20:18 file1
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zfs snapshot">zfs snapshot</h5>
    <p>Создать snapshot под названием test-27-01-2018</p>

</td><td>

<pre><code class="shell">
# zfs snapshot test01/test02@test-27-01-2018
</code></pre>

<p>Для вывода снапшотов нужно либо:</p>
<p>использовать ключ <b>-t snapshot</b></p>

<pre><code class="shell">
# zfs list -t snapshot
NAME                            USED  AVAIL  REFER  MOUNTPOINT
test01/test02@test-27-01-2018      0      -  50.1M  -
</code></pre>

<p>Либо установить у пула параметр <b>listsnapshot=on</b></p>

<pre><code class="shell">
# zpool get listsnapshots test01
NAME    PROPERTY       VALUE      SOURCE
test01  listsnapshots  off        default
# zpool set listsnapshots=on test01
# zfs list
NAME                            USED  AVAIL  REFER  MOUNTPOINT
test01                         50.1M  77.9M    19K  /test01
test01/test02                  50.1M  77.9M  50.1M  /test01/test02
test01/test02@test-27-01-2018      0      -  50.1M  -
</code></pre>

</td></tr>
<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zfs rollback">zfs rollback</h5>
    <p>Откатиться на снапшот</p>

</td><td>

<pre><code class="shell">
# ls -lh /test01/test02/
total 51240
-rw-r--r--  1 root  wheel    50M Jan 27 21:02 file1
# echo "hi" > /test01/test02/test
# ls -lh /test01/test02/
total 51240
-rw-r--r--  1 root  wheel    50M Jan 27 21:02 file1
-rw-r--r--  1 root  wheel     3B Jan 27 21:21 test
# zfs list
NAME                            USED  AVAIL  REFER  MOUNTPOINT
test01                         50.2M  77.8M    19K  /test01
test01/test02                  50.1M  77.8M  50.1M  /test01/test02
test01/test02@test-27-01-2018     9K      -  50.1M  -
# zfs rollback test01/test02@test-27-01-2018
# ls -lh /test01/test02/
total 51240
-rw-r--r--  1 root  wheel    50M Jan 27 21:02 file1
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zfs clone">zfs clone</h5>
    <p>Клонирование фс. </p>
    <p>Также используется для просмотра содержимого снапшота</p>

</td><td>

<pre><code class="shell">
# rm /test01/test02/file1 
# echo "hi" >> /test01/test02/file2
# zfs list
NAME                            USED  AVAIL  REFER  MOUNTPOINT
test01                         50.2M  77.8M    19K  /test01
test01/test02                  50.1M  77.8M  19.5K  /test01/test02
test01/test02@test-27-01-2018  50.0M      -  50.1M  -
# zfs clone test01/test02@test-27-01-2018 test01/test03 
# ls -lhR /test01/*
/test01/test02:
total 1
-rw-r--r--  1 root  wheel     3B Jan 27 21:30 file2

/test01/test03:
total 51240
-rw-r--r--  1 root  wheel    50M Jan 27 21:02 file1
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zfs destroy">zfs destroy</h5>
    <p>Уничтожение фс</p>
    
</td><td>

<pre><code class="shell">
# zfs list    
NAME                            USED  AVAIL  REFER  MOUNTPOINT
test01                         50.2M  77.8M    19K  /test01
test01/test02                  50.1M  77.8M  50.1M  /test01/test02
test01/test02@test-27-01-2018     9K      -  50.1M  -
test01/test03                     9K  77.8M  50.1M  /test01/test03
[root@freebsd2 ~]# zfs destroy test01/test03
</code></pre>

<p>Если в фс есть вложенные фс, то будет предложенно удалить рекурсивно</p>

<pre><code class="shell">
# zfs destroy test01/test02
cannot destroy 'test01/test02': filesystem has children
use '-r' to destroy the following datasets:
test01/test02@test-27-01-2018
# zfs destroy -r test01/test02
# zfs list
NAME     USED  AVAIL  REFER  MOUNTPOINT
test01   103K   128M    19K  /test01
</code></pre>

<p>Также будут удалены связанные снапшоты</p>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="zfs promote">zfs promote</h5>
    <p>Вынисение клонированной фс</p>
    <p>Переключение всех снапшотов с оригинальной фс, на копию</p>

</td><td>

<pre><code class="shell">
# zfs list
NAME                       USED  AVAIL  REFER  MOUNTPOINT
test01                     123M  4.61M    19K  /test01
test01/test02              123M  4.61M   123M  /test01/test02
test01/test02@27-01-2018      0      -   123M  -
# zfs clone test01/test02@27-01-2018 test01/test03
# zfs destroy test01/test02@27-01-2018
cannot destroy 'test01/test02@27-01-2018': snapshot has dependent clones
use '-R' to destroy the following datasets:
test01/test03@27-01-2018
test01/test03


</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

<tr><td>

    <h5 id="test">test</h5>
    <p>test</p>

</td><td>

<pre><code class="shell">
</code></pre>

</td></tr>

<!-- ------------------------------------------------------------- -->

</table>

### Эксперименты
<h5 id="1">№ 1</h5>
<p>4 файла по 100мб</p>
<pre><code class="shell">cd /
# mkfile 100m disk1 disk2 disk3 disk4
# ls -lh /disk*
-rw-------  1 root  wheel   100M Jan 27 18:27 /disk1
-rw-------  1 root  wheel   100M Jan 27 18:27 /disk2
-rw-------  1 root  wheel   100M Jan 27 18:27 /disk3
-rw-------  1 root  wheel   100M Jan 27 18:27 /disk4
</code></pre>

<p>Первый зеркальный пул - size 80мб</p>
<pre><code class="shell">
# zpool create test01 mirror /disk1 /disk2 
# zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test01    80M    62K  79.9M         -     1%     0%  1.00x  ONLINE  -
</code></pre>

<p>Помещается 48мб, еще свободно 32мб но создать уже ничего не получается!</p>
<pre><code class="shell">
# cd /test01/
root@freebsd2:/test01 # cat /dev/urandom > test
cat: stdout: No space left on device
root@freebsd2:/test01 # ls -lh 
total 49065
-rw-r--r--  1 root  wheel    48M Jan 27 18:31 test
root@freebsd2:/test01 # zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test01    80M  48.0M  32.0M         -    44%    60%  1.00x  ONLINE  -
# touch test1
touch: test1: No space left on device
</code></pre>

<p>Вопрос почему?</p>

<h5 id="2">№ 2</h5>

<p>Пул из 2 обьединенных дисков по 100мб, size - 160мб</p>
<pre><code class="shell">
# zpool create test02 /disk3 /disk4 
root@freebsd2:/test01 # zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test02   160M   257K   160M         -     1%     0%  1.00x  ONLINE  -
</code></pre>

<p>Помещается 128мб, свободно опять осталось 32мб</p>
<pre><code class="shell">
# cd /test02/
# cat /dev/urandom > test
cat: stdout: No space left on device
root@freebsd2:/test02 # ls -lh
total 131041
-rw-r--r--  1 root  wheel   128M Jan 27 18:37 test
# zpool list
NAME     SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
test02   160M   128M  31.9M         -    60%    80%  1.00x  ONLINE  -
</code></pre>

<h5 id="3">№ 3</h5>

<p>Пул из примера <a href="#2">2</a>, размер 160мб, реально вмещается 128мб</p>
<p>Зарезервируем 30мб и посмотрим сколько вместиться</p>

<pre><code class="shell">
# zfs set reservation=30m test02
root@freebsd2:~ # cd /test02
root@freebsd2:/test02 # cat /dev/urandom > test
cat: stdout: No space left on device
root@freebsd2:/test02 # ls -lh
total 131041
-rw-r--r--  1 root  wheel   128M Jan 27 19:42 test
root@freebsd2:/test02 # zfs list -o reservation
RESERV
   30M
</code></pre>

<p>Хоть и зарезервировали но сожрали все место</p>

<p>Попробуем добавить вложенную фс и установить для нее резервирование</p>

<pre><code class="shell">
# zfs create test02/test03
# zfs set reservation=30m test02/test03
# cat /dev/urandom > /test02/file
cat: stdout: No space left on device
# ls -lh /test02/
total 100174
-rw-r--r--  1 root  wheel    98M Jan 27 19:55 file
drwxr-xr-x  2 root  wheel     2B Jan 27 19:55 test03
</code></pre>

<p>Урааа получилось! но почему-то использовать мы можем не все 30мб</p>

<pre><code class="shell">
# zfs list
NAME            USED  AVAIL  REFER  MOUNTPOINT
test02          128M  41.5K  97.8M  /test02
test02/test03    19K  30.0M    19K  /test02/test03
# cat /dev/urandom > /test02/test03/file2
cat: stdout: No space left on device
# ls -lh /test02/test03
total 7943
-rw-r--r--  1 root  wheel   7.8M Jan 27 19:56 file2
</code></pre>

<p>Даже df говорит что у меня есть еще 22мб, но даже временную метку не получается создать</p>

<pre><code class="shell">
# df -h
Filesystem       Size    Used   Avail Capacity  Mounted on
/dev/ada0p2      9.0G    4.5G    3.7G    55%    /
devfs            1.0K    1.0K      0B   100%    /dev
test02            98M     98M      0B   100%    /test02
test02/test03     30M    7.8M     22M    26%    /test02/test03
# touch /test02/test03/hello
touch: /test02/test03/hello: No space left on device
</code></pre>

<h5 id="4">№ 4</h5>

<p>Создаем 2 фс</p>

<pre><code class="shell">
# dd if=/dev/urandom of=big_file bs=10M count=5
5+0 records in
5+0 records out
52428800 bytes transferred in 1.894308 secs (27677022 bytes/sec)
root@freebsd2:~ # cp big_file /test01/test04/ /test02/test03/
root@freebsd2:~ # ls -lh /test01/test04/ /test02/test03/
/test01/test04/:
total 51240
-rw-r--r--  1 root  wheel    50M Jan 27 20:36 big_file

/test02/test03/:
total 51240
-rw-r--r--  1 root  wheel    50M Jan 27 20:36 big_file
root@freebsd2:~ # df -h 
Filesystem       Size    Used   Avail Capacity  Mounted on
/dev/ada0p2      9.0G    4.6G    3.7G    55%    /
devfs            1.0K    1.0K      0B   100%    /dev
test01            78M     19K     78M     0%    /test01
test02            78M     19K     78M     0%    /test02
test02/test03    128M     50M     78M    39%    /test02/test03
test01/test04    128M     50M     78M    39%    /test01/test04
</code></pre>

Чет без разницы
